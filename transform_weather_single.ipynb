{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, datetime\n",
    "import time\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "from __future__ import absolute_import\n",
    "from apache_beam import pvalue\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Dataset uber-fair:temp_dataset_77ec28a7b411479fb25591993c7cc24a does not exist so we will create it as temporary with location=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1543546374\n",
      "{u'time_stamp': '2018-11-30', u'id': u'a962c95c-caf1-4aa7-bd16-47235ea56e65'}\n",
      "1543546374\n",
      "{u'time_stamp': '2018-11-30', u'id': u'b041d6c2-c850-4c42-a5f7-7a29020d8f44'}\n",
      "1543546374\n",
      "{u'time_stamp': '2018-11-30', u'id': u'3463b2f2-6326-4237-bf17-df2301e16e83'}\n",
      "1543546374\n",
      "{u'time_stamp': '2018-11-30', u'id': u'0533eb82-e9ac-43a5-beb5-7c1ce3cfd529'}\n",
      "1543546374\n",
      "{u'time_stamp': '2018-11-30', u'id': u'd64d75db-87cd-452b-8bc7-6dcaf0a6db1e'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Sleeping for 150 seconds before the write as BigQuery inserts can be routed to deleted table for 2 mins after the delete and create.\n",
      "ERROR:root:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7fcdf2602128>, due to an exception.\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/jupyter/beam_venv_dir/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py\", line 341, in call\n",
      "    finish_state)\n",
      "  File \"/home/jupyter/beam_venv_dir/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py\", line 381, in attempt_call\n",
      "    result = evaluator.finish_bundle()\n",
      "  File \"/home/jupyter/beam_venv_dir/local/lib/python2.7/site-packages/apache_beam/runners/direct/transform_evaluator.py\", line 577, in finish_bundle\n",
      "    self.runner.finish()\n",
      "  File \"apache_beam/runners/common.py\", line 598, in apache_beam.runners.common.DoFnRunner.finish\n",
      "    self._invoke_bundle_method(self.do_fn_invoker.invoke_finish_bundle)\n",
      "  File \"apache_beam/runners/common.py\", line 589, in apache_beam.runners.common.DoFnRunner._invoke_bundle_method\n",
      "    self._reraise_augmented(exn)\n",
      "  File \"apache_beam/runners/common.py\", line 618, in apache_beam.runners.common.DoFnRunner._reraise_augmented\n",
      "    six.reraise(type(new_exn), new_exn, original_traceback)\n",
      "  File \"apache_beam/runners/common.py\", line 587, in apache_beam.runners.common.DoFnRunner._invoke_bundle_method\n",
      "    bundle_method()\n",
      "  File \"apache_beam/runners/common.py\", line 299, in apache_beam.runners.common.DoFnInvoker.invoke_finish_bundle\n",
      "    def invoke_finish_bundle(self):\n",
      "  File \"apache_beam/runners/common.py\", line 303, in apache_beam.runners.common.DoFnInvoker.invoke_finish_bundle\n",
      "    self.signature.finish_bundle_method.method_value())\n",
      "  File \"/home/jupyter/beam_venv_dir/local/lib/python2.7/site-packages/apache_beam/io/gcp/bigquery.py\", line 1270, in finish_bundle\n",
      "    self._flush_batch()\n",
      "  File \"/home/jupyter/beam_venv_dir/local/lib/python2.7/site-packages/apache_beam/io/gcp/bigquery.py\", line 1282, in _flush_batch\n",
      "    self.table_id, errors))\n",
      "RuntimeError: Could not successfully insert rows to BigQuery table [uber-fair:weather_modeled.weather_DR]. Errors: [<InsertErrorsValueListEntry\n",
      " errors: [<ErrorProto\n",
      " debugInfo: u''\n",
      " location: u'id'\n",
      " message: u'no such field.'\n",
      " reason: u'invalid'>]\n",
      " index: 0>, <InsertErrorsValueListEntry\n",
      " errors: [<ErrorProto\n",
      " debugInfo: u''\n",
      " location: u'id'\n",
      " message: u'no such field.'\n",
      " reason: u'invalid'>]\n",
      " index: 1>, <InsertErrorsValueListEntry\n",
      " errors: [<ErrorProto\n",
      " debugInfo: u''\n",
      " location: u'id'\n",
      " message: u'no such field.'\n",
      " reason: u'invalid'>]\n",
      " index: 2>, <InsertErrorsValueListEntry\n",
      " errors: [<ErrorProto\n",
      " debugInfo: u''\n",
      " location: u'id'\n",
      " message: u'no such field.'\n",
      " reason: u'invalid'>]\n",
      " index: 3>, <InsertErrorsValueListEntry\n",
      " errors: [<ErrorProto\n",
      " debugInfo: u''\n",
      " location: u'id'\n",
      " message: u'no such field.'\n",
      " reason: u'invalid'>]\n",
      " index: 4>] [while running 'Write BQ table/WriteToBigQuery']\n",
      "\n",
      "ERROR:root:Giving up after 4 attempts.\n",
      "WARNING:root:A task failed with exception: Could not successfully insert rows to BigQuery table [uber-fair:weather_modeled.weather_DR]. Errors: [<InsertErrorsValueListEntry\n",
      " errors: [<ErrorProto\n",
      " debugInfo: u''\n",
      " location: u'id'\n",
      " message: u'no such field.'\n",
      " reason: u'invalid'>]\n",
      " index: 0>, <InsertErrorsValueListEntry\n",
      " errors: [<ErrorProto\n",
      " debugInfo: u''\n",
      " location: u'id'\n",
      " message: u'no such field.'\n",
      " reason: u'invalid'>]\n",
      " index: 1>, <InsertErrorsValueListEntry\n",
      " errors: [<ErrorProto\n",
      " debugInfo: u''\n",
      " location: u'id'\n",
      " message: u'no such field.'\n",
      " reason: u'invalid'>]\n",
      " index: 2>, <InsertErrorsValueListEntry\n",
      " errors: [<ErrorProto\n",
      " debugInfo: u''\n",
      " location: u'id'\n",
      " message: u'no such field.'\n",
      " reason: u'invalid'>]\n",
      " index: 3>, <InsertErrorsValueListEntry\n",
      " errors: [<ErrorProto\n",
      " debugInfo: u''\n",
      " location: u'id'\n",
      " message: u'no such field.'\n",
      " reason: u'invalid'>]\n",
      " index: 4>] [while running 'Write BQ table/WriteToBigQuery']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DONE'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CastAsDateTime(beam.DoFn):\n",
    "  def process(self, element):\n",
    "    wData = element\n",
    "    print(wData[\"time_stamp\"])\n",
    "    weatherDate = str(time.strftime('%Y-%m-%d', time.localtime(wData[\"time_stamp\"])))\n",
    "    wData['time_stamp'] = weatherDate\n",
    "    print(wData)\n",
    "    return [wData]\n",
    "           \n",
    "         \n",
    "PROJECT_ID = 'uber-fair'\n",
    "\n",
    "# Project ID is required when using the BQ source\n",
    "options = {\n",
    "    'project': PROJECT_ID,\n",
    "}\n",
    "opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "\n",
    "# Create beam pipeline using local runner\n",
    "p = beam.Pipeline('DirectRunner', options=opts)\n",
    "\n",
    "sql = 'SELECT time_stamp,id FROM weather_modeled.weather LIMIT 5'\n",
    "bq_source = beam.io.BigQuerySource(query=sql, use_standard_sql=True)\n",
    "\n",
    "query_results = p | 'Read from BigQuery' >> beam.io.Read(bq_source)\n",
    "\n",
    "# write PCollection to log file\n",
    "query_results | 'Write log 1' >> WriteToText('query_results.txt')\n",
    "\n",
    "# Par do to the date\n",
    "date_pcoll = query_results | 'Fix Date parDo' >> beam.ParDo(CastAsDateTime())\n",
    "\n",
    "#write the output table to txt\n",
    "date_pcoll | 'Write log 2' >> WriteToText('fixed_date_pcoll.txt')\n",
    "\n",
    "dataset_id = 'weather_modeled'\n",
    "table_id = 'weather_DR'\n",
    "schema_id = 'time_stamp:DATE, id:STRING'\n",
    "\n",
    "# write PCollection to new BQ table\n",
    "date_pcoll | 'Write BQ table' >> beam.io.WriteToBigQuery(dataset=dataset_id,\n",
    "                                                table=table_id,\n",
    "                                                schema=schema_id,\n",
    "                                                project=PROJECT_ID,\n",
    "                                                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                                                write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE,\n",
    "                                                batch_size=int(10000000))\n",
    "result = p.run()\n",
    "result.wait_until_finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (beam_venv)",
   "language": "python",
   "name": "beam_venv_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
